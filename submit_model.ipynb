{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的文件读取路径可能得修改一下..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取个人信息\n",
    "train_agg = pd.read_csv('data/train_agg.csv',sep='\\t')\n",
    "test_agg = pd.read_csv('data/test_agg.csv',sep='\\t')\n",
    "agg = pd.concat([train_agg,test_agg],copy=False)\n",
    "\n",
    "# 日志信息\n",
    "train_log = pd.read_csv('data/train_log.csv',sep='\\t')\n",
    "test_log = pd.read_csv('data/test_log.csv',sep='\\t')\n",
    "df_log = pd.concat([train_log,test_log],copy=False)\n",
    "\n",
    "\n",
    "# 用户唯一标识\n",
    "train_flg = pd.read_csv('data/train_flg.csv',sep='\\t')\n",
    "test_flg = pd.read_csv('data/submit_sample.csv',sep='\\t')\n",
    "test_flg['FLAG'] = -1\n",
    "del(test_flg['RST'])\n",
    "flg = pd.concat([train_flg,test_flg],copy=False)\n",
    "\n",
    "data = pd.merge(agg,flg,on=['USRID'],how='left',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(agg,flg,on=['USRID'],how='left',copy=False)\n",
    "\n",
    "# 这里对agg表稍微做了处理，因为统计到V2,V4,V5就2种类型，可能之一为年龄或者等价为年龄的特征，然后V26的总类型数目，我认为是年龄，\n",
    "# 然后将其按照下面的bins切分，并和V2,V4,V5组合起来，切分成中年男、少年男、老年男等等。当然下面的V22也有可能是类似年龄的特征，\n",
    "# 也做了相同处理。\n",
    "\n",
    "\n",
    "bins = [-1,1,2.3,5,11,100]\n",
    "a = pd.cut(data['V26'],bins)\n",
    "label = LabelEncoder()\n",
    "data['V26'] = label.fit_transform(a)\n",
    "\n",
    "#组合特征\n",
    "data['V4_V26'] = data['V4'] + data['V26'] \n",
    "label = LabelEncoder()\n",
    "data['V4_V26'] = label.fit_transform(data['V4_V26'])\n",
    "\n",
    "data['V2_V26'] = data['V2'] + data['V26'] \n",
    "label = LabelEncoder()\n",
    "data['V2_V26'] = label.fit_transform(data['V2_V26'])\n",
    "\n",
    "data['V5_V26'] = data['V5'] + data['V26'] \n",
    "label = LabelEncoder()\n",
    "data['V5_V26'] = label.fit_transform(data['V5_V26'])\n",
    "\n",
    "bins = [-1,0,1,2.3,4,10,100]\n",
    "a = pd.cut(data['V22'],bins)\n",
    "label = LabelEncoder()\n",
    "data['V22'] = label.fit_transform(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先统一把时间换算为秒，便于后面的计算\n",
    "\n",
    "import time\n",
    "log = pd.concat([train_log,test_log],copy=False)\n",
    "a = log['OCC_TIM'].apply(lambda x:time.mktime(time.strptime(x, \"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "date_1_time = '2018-04-01 00:00:00'\n",
    "date_8_time = '2018-04-08 00:00:00'\n",
    "b_1 = time.mktime(time.strptime(date_1_time, \"%Y-%m-%d %H:%M:%S\"))\n",
    "b_8 = time.mktime(time.strptime(date_8_time, \"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.concat([train_log,test_log],copy=False)\n",
    "\n",
    "# 这个部分计算当前点击APP的时间距离4月1号和4月8号的时间差的统计特征，如均值、方差、最大、最小、中位数、偏度、峰度。这些统计特征\n",
    "# 可以充分挖掘到不同用户的行为差异。\n",
    "\n",
    "\n",
    "log['OCC_TIM_1'] = (a - b_1).apply(np.abs)\n",
    "m = log.groupby(['USRID'],as_index=False)['OCC_TIM_1'].agg({\n",
    "    'OCC_TIM_1_mean':np.mean,\n",
    "    'OCC_TIM_1_std':np.std,\n",
    "    'OCC_TIM_1_min':np.min,\n",
    "    'OCC_TIM_1_max':np.max,\n",
    "    'OCC_TIM_1_median':np.median,\n",
    "    'OCC_TIM_1_skew':skew,\n",
    "    'OCC_TIM_1_kurtosis':kurtosis\n",
    "})\n",
    "data = pd.merge(data,m,on=['USRID'],how='left',copy=False)\n",
    "\n",
    "log['OCC_TIM_8'] = (a - b_8).apply(np.abs)\n",
    "n = log.groupby(['USRID'],as_index=False)['OCC_TIM_8'].agg({\n",
    "    'OCC_TIM_8_mean':np.mean,\n",
    "    'OCC_TIM_8_std':np.std,\n",
    "    'OCC_TIM_8_min':np.min,\n",
    "    'OCC_TIM_8_max':np.max,\n",
    "    'OCC_TIM_8_median':np.median,\n",
    "    'OCC_TIM_8_skew':skew,\n",
    "    'OCC_TIM_8_kurtosis':kurtosis\n",
    "})\n",
    "data = pd.merge(data,n,on=['USRID'],how='left',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.concat([train_log,test_log],copy=False)\n",
    "\n",
    "# 这个部分利用点击的APP时间进行排序。然后计算用户下一次点击APP的时间差统计特征\n",
    "\n",
    "log['OCC_TIM'] = a\n",
    "log = log.sort_values(['USRID','OCC_TIM'])\n",
    "log['next_time'] = log.groupby(['USRID'])['OCC_TIM'].diff(-1).apply(np.abs)\n",
    "\n",
    "p = log.groupby(['USRID'],as_index=False)['next_time'].agg({\n",
    "    'next_time_mean':np.mean,\n",
    "    'next_time_std':np.std,\n",
    "    'next_time_min':np.min,\n",
    "    'next_time_max':np.max,\n",
    "    'next_time_median':np.median,\n",
    "    'next_time_skew':skew,\n",
    "    'next_time_kurtosis':kurtosis\n",
    "})\n",
    "\n",
    "data = pd.merge(data,p,on=['USRID'],how='left',copy=False)\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.merge(df_log, data[['USRID','FLAG']], on='USRID')\n",
    "\n",
    "#对于点击模块问题，虽然为数字编码，但是题目意思是说点击模块的3个级别，是存在大小关系的。所以我进行了类似于上述时间的统计特征的计算。\n",
    "#不过在计算之前，是先将3个模块分开，分别计算每个模块的统计特征。\n",
    "\n",
    "new['EVT_LBL_1'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[0]))\n",
    "new['EVT_LBL_2'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[1]))\n",
    "new['EVT_LBL_3'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[2]))\n",
    "\n",
    "new_1 = new.groupby(['USRID'],as_index=False)['EVT_LBL_1'].agg({\n",
    "    'EVT_LBL_1_mean':np.mean,\n",
    "    'EVT_LBL_1_std':np.std,\n",
    "    'EVT_LBL_1min':np.min,\n",
    "    'EVT_LBL_1_max':np.max,\n",
    "    'EVT_LBL_1_median':np.median,\n",
    "    'EVT_LBL_1_skew':skew,\n",
    "    'EVT_LBL_1_kurtosis':kurtosis\n",
    "})\n",
    "data = pd.merge(data,new_1,on=['USRID'],how='left',copy=False)\n",
    "\n",
    "new_2 = new.groupby(['USRID'],as_index=False)['EVT_LBL_2'].agg({\n",
    "    'EVT_LBL_2_mean':np.mean,\n",
    "    'EVT_LBL_2_std':np.std,\n",
    "    'EVT_LBL_2min':np.min,\n",
    "    'EVT_LBL_2_max':np.max,\n",
    "    'EVT_LBL_2_median':np.median,\n",
    "    'EVT_LBL_2_skew':skew,\n",
    "    'EVT_LBL_2_kurtosis':kurtosis\n",
    "})\n",
    "data = pd.merge(data,new_2,on=['USRID'],how='left',copy=False)\n",
    "\n",
    "new_3 = new.groupby(['USRID'],as_index=False)['EVT_LBL_3'].agg({\n",
    "    'EVT_LBL_3_mean':np.mean,\n",
    "    'EVT_LBL_3_std':np.std,\n",
    "    'EVT_LBL_3min':np.min,\n",
    "    'EVT_LBL_3_max':np.max,\n",
    "    'EVT_LBL_3_median':np.median,\n",
    "    'EVT_LBL_3_skew':skew,\n",
    "    'EVT_LBL_3_kurtosis':kurtosis\n",
    "})\n",
    "data = pd.merge(data,new_3,on=['USRID'],how='left',copy=False)\n",
    "\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.merge(df_log, data[['USRID','FLAG']], on='USRID')\n",
    "\n",
    "#这个模块统计了每个有log的用户分别点击了多少种模块\n",
    "\n",
    "new['EVT_LBL_1'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[0]))\n",
    "new['EVT_LBL_2'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[1]))\n",
    "new['EVT_LBL_3'] = new['EVT_LBL'].apply(lambda x:int(x.split('-')[2]))\n",
    "\n",
    "a1 = new.groupby('USRID', as_index=False)['EVT_LBL_1'].count()\n",
    "data = pd.merge(data, a1, on='USRID', how='left')\n",
    "\n",
    "a2 = new.groupby('USRID', as_index=False)['EVT_LBL_2'].count()\n",
    "data = pd.merge(data, a2, on='USRID', how='left')\n",
    "\n",
    "a3 = new.groupby('USRID', as_index=False)['EVT_LBL_3'].count()\n",
    "data = pd.merge(data, a3, on='USRID', how='left')\n",
    "\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#加入每位顾客点击APP次数的特征\n",
    "c= pd.DataFrame()\n",
    "c['USRID'] = df_log.USRID.value_counts().index\n",
    "c['click_total'] = df_log.USRID.value_counts().values\n",
    "data = pd.merge(data,c,on=['USRID'],how='left',copy=False)\n",
    "data.loc[data['click_total'].isnull(), 'click_total'] = 0\n",
    "\n",
    "#加入顾客是如何点击的特征，两种方式：0  2, 缺失值的顾客将其记为-1\n",
    "b = df_log.groupby(['USRID'], as_index=False)['TCH_TYP'].sum()\n",
    "data = pd.merge(data, b, on = 'USRID', how='left')\n",
    "data.loc[data.TCH_TYP > 0, 'TCH_TYP'] = 2.0\n",
    "data['TCH_TYP'].loc[data['TCH_TYP'].isnull()] = -1\n",
    "le = LabelEncoder()\n",
    "data['TCH_TYP'] = le.fit_transform(data['TCH_TYP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个特征是顾客在3月双休日点击APP的总次数\n",
    "new = pd.merge(df_log, data[['USRID','FLAG']], on='USRID')\n",
    "\n",
    "new['date'] = new.OCC_TIM.apply(lambda x:x.split()[0])\n",
    "new['date'] = new['date'].apply(lambda x:x.split('-')[2])\n",
    "new['date_week'] = new.date.apply(lambda x:1 if x in ['03','04','17', '18','24','25','31'] else 0)\n",
    "a = new.groupby('USRID', as_index=False)['date_week'].sum()\n",
    "data = pd.merge(data, a, on='USRID',how='left')\n",
    "\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上面漏了一个agg的处理，也就是将V12进行7等分等频率切割，V2、V4、V5分别两两组合以下作为新的特征\n",
    "\n",
    "data['V12'] = pd.qcut(data['V12'], 7)\n",
    "label = LabelEncoder()\n",
    "data['V12'] = label.fit_transform(data['V12'])\n",
    "\n",
    "#组合特征\n",
    "data['V4_V5'] = data['V4'] + data['V5'] \n",
    "data['V2_V4'] = data['V2'] + data['V4'] \n",
    "data['V2_V5'] = data['V2'] + data['V5'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这些特征是为了计算用户点击APP的时间段，比如晚上22点到凌晨1点这个时间段用户点击APP的总次数。\n",
    "#分别考虑了时针和秒针。\n",
    "\n",
    "new['date_h_1'] = new.OCC_TIM.apply(lambda x:x.split()[1])\n",
    "new['date_h_1'] = new['date_h_1'].apply(lambda x:x.split(':')[0])\n",
    "new['date_h_1'] = new['date_h_1'].apply(lambda x:1 if int(x) < 1 else 0)\n",
    "\n",
    "new['date_h_2'] = new.OCC_TIM.apply(lambda x:x.split()[1])\n",
    "new['date_h_2'] = new['date_h_2'].apply(lambda x:x.split(':')[0])\n",
    "new['date_h_2'] = new['date_h_2'].apply(lambda x:1 if int(x) > 22 else 0)\n",
    "\n",
    "new['date_m_1'] = new.OCC_TIM.apply(lambda x:x.split()[1])\n",
    "new['date_m_1'] = new['date_m_1'].apply(lambda x:x.split(':')[1])\n",
    "new['date_m_1'] = new['date_m_1'].apply(lambda x:1 if int(x) < 4 else 0)\n",
    "\n",
    "new['date_m_2'] = new.OCC_TIM.apply(lambda x:x.split()[1])\n",
    "new['date_m_2'] = new['date_m_2'].apply(lambda x:x.split(':')[1])\n",
    "new['date_m_2'] = new['date_m_2'].apply(lambda x:1 if int(x) > 57 else 0)\n",
    "\n",
    "\n",
    "new['date_add_h'] =new['date_h_2']+new['date_h_1']\n",
    "a1 = new.groupby('USRID', as_index=False)['date_add_h'].sum()\n",
    "data = pd.merge(data, a1, on='USRID',how='left')\n",
    "\n",
    "new['date_add_m'] =new['date_m_2'] + new['date_m_1']\n",
    "a1 = new.groupby('USRID', as_index=False)['date_add_m'].sum()\n",
    "data = pd.merge(data, a1, on='USRID',how='left')\n",
    "\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80000, 88)\n",
      "test (20000, 88)\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's auc: 0.843659\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's auc: 0.857161\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's auc: 0.854371\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's auc: 0.850166\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's auc: 0.854849\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's auc: 0.854392\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's auc: 0.849216\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid_0's auc: 0.865288\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's auc: 0.8474\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's auc: 0.853611\n",
      "Start predicting...\n"
     ]
    }
   ],
   "source": [
    "# 上面就是模型的总特征，下面提取测试集和训练集合，用lightgbm进行预测，由于交叉验证的折数不好确定，我将模型分为5折和10折，然后对两个结果\n",
    "# 取平均作为最后预测的结果。\n",
    "\n",
    "train = data.loc[(data['FLAG']!=-1)]\n",
    "test = data.loc[(data['FLAG']==-1)]\n",
    "print('train',train.shape)\n",
    "print('test',test.shape)\n",
    "\n",
    "# 构造数据\n",
    "# 提取userid和单独把标签赋值一个变量\n",
    "train_userid = train.pop('USRID')\n",
    "y = train.pop('FLAG')\n",
    "col = train.columns\n",
    "X = train[col].values\n",
    "\n",
    "test_userid = test.pop('USRID')\n",
    "test_y = test.pop('FLAG')\n",
    "test_X = test[col].values\n",
    "\n",
    "#归一化\n",
    "std_scaler = StandardScaler()\n",
    "X = std_scaler.fit_transform(X)\n",
    "test_X = std_scaler.transform(test_X)\n",
    "\n",
    "N = 10\n",
    "skf = StratifiedKFold(n_splits=N,shuffle=False,random_state=42)\n",
    "\n",
    "xx_cv = []\n",
    "xx_pre = []\n",
    "\n",
    "import operator\n",
    "\n",
    "for k,(train_in,test_in) in enumerate(skf.split(X,y)):\n",
    "    X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\n",
    "\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'auc'},\n",
    "        'num_leaves': 32,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=40000,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=50,\n",
    "                   verbose_eval=1500)\n",
    "\n",
    "    print('Start predicting...')\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    xx_cv.append(roc_auc_score(y_test,np.array(y_pred)))\n",
    "    xx_pre.append(gbm.predict(test_X, num_iteration=gbm.best_iteration))\n",
    "\n",
    "xx_pre_yu_10 = np.mean(xx_pre, axis=0)\n",
    "xx_cv_10 = np.mean(xx_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (80000, 88)\n",
      "test (20000, 88)\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's auc: 0.852038\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's auc: 0.853457\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's auc: 0.853138\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's auc: 0.852796\n",
      "Start predicting...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's auc: 0.848554\n",
      "Start predicting...\n"
     ]
    }
   ],
   "source": [
    "# 提取测试集和训练集合\n",
    "\n",
    "train = data.loc[(data['FLAG']!=-1)]\n",
    "test = data.loc[(data['FLAG']==-1)]\n",
    "print('train',train.shape)\n",
    "print('test',test.shape)\n",
    "\n",
    "# 构造数据\n",
    "# 提取userid和单独把标签赋值一个变量\n",
    "train_userid = train.pop('USRID')\n",
    "y = train.pop('FLAG')\n",
    "col = train.columns\n",
    "X = train[col].values\n",
    "\n",
    "test_userid = test.pop('USRID')\n",
    "test_y = test.pop('FLAG')\n",
    "test_X = test[col].values\n",
    "\n",
    "#还是需要归一化的\n",
    "std_scaler = StandardScaler()\n",
    "X = std_scaler.fit_transform(X)\n",
    "test_X = std_scaler.transform(test_X)\n",
    "\n",
    "N = 5\n",
    "skf = StratifiedKFold(n_splits=N,shuffle=False,random_state=42)\n",
    "\n",
    "xx_cv = []\n",
    "xx_pre = []\n",
    "\n",
    "import operator\n",
    "\n",
    "for k,(train_in,test_in) in enumerate(skf.split(X,y)):\n",
    "    X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\n",
    "\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'auc'},\n",
    "        'num_leaves': 32,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=40000,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=50,\n",
    "                   verbose_eval=1500)\n",
    "\n",
    "    print('Start predicting...')\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    xx_cv.append(roc_auc_score(y_test,np.array(y_pred)))\n",
    "    xx_pre.append(gbm.predict(test_X, num_iteration=gbm.best_iteration))\n",
    "\n",
    "xx_pre_yu = np.mean(xx_pre, axis=0)\n",
    "xx_cv = np.mean(xx_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_pre_yu = (xx_pre_yu + xx_pre_yu_10) / 2\n",
    "result = xx_pre_yu\n",
    "res = pd.DataFrame()\n",
    "res['USRID'] = list(test_userid.values)\n",
    "res['RST'] = list(result)\n",
    "\n",
    "time_date = time.strftime('%Y-%m-%d',time.localtime(time.time()))\n",
    "res.to_csv('%s_%s.csv'%(str(time_date),str((xx_cv + xx_cv_10)/2).split('.')[1]),index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
